LLM_PROVIDER=ollama
# LLM_PROVIDER=groq

LLM_API_KEY=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

#Ollama specific
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_HOST=http://host.docker.internal:11434


#Groq specific
# GROQ_MODEL='qwen-2.5-coder-32b' #expensive
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct


#implementation for openai pending