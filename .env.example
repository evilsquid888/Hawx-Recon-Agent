
LLM_API_KEY=XXXXXX

# LLM_PROVIDER=ollama
LLM_PROVIDER=groq
# MODEL=phi3
MODEL=meta-llama/llama-4-scout-17b-16e-instruct

LLM_CONTEXT_LENGTH=8192
# LM_CONTEXT_LENGTH=60000
#Some models will have larger context length
#Unfortunately there is no way to dynamically fetch this

#Ollama specific
OLLAMA_HOST=http://host.docker.internal:11434


#implementation for openai pending
